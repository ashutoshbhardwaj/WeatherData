{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program: Weather Test Data Generator\n",
    "# Date : 1st July, 2018\n",
    "# Author : Ashutosh Bhardwaj\n",
    "\n",
    "## Narrative: \n",
    "\n",
    "1. This application takes any number of coordinates(Latitude, Longitude) within Australia as an input file with format -\n",
    "                        place,state,latitude,longitude\n",
    "                        ------------------ ----------\n",
    "                        Abbeywood,QLD,-26.10688,151.6288\n",
    "\n",
    "    For sake of convienence, provided a list of towns(some 400+) with their latitudes and longitudes, downloaded from https://www.australiantownslist.com/. One can create your own list of arbitrary number of coordinates.\n",
    "\n",
    "2. Weather data for 49 Australian stations dataset from Kaggle - https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "The coordinates(latitude,langitude) of weather stations are collected using Google API. Seperate program for that. \n",
    "\n",
    "3. Then it  calculate the distance of given list of coordinates from the coordinates of Weather Stations(49 precisely) fetched from reference file (yyyy) using Haversine formula. The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes. Details - https://en.wikipedia.org/wiki/Haversine_formula\n",
    "\n",
    "4. Sort the list with computed distance of given coordinates from different weather staions(49 precisely) and pick the closest weather station to the given coordinate.Now you have list of provided coordinates with their closest weather station. \n",
    "\n",
    "5. In the dataframe generated in step 3, add another column with random date.\n",
    "\n",
    "6. For the random date, read the Historical Weather data emitted for particular station on that random date.\n",
    "\n",
    "7. Based on the input provided by the weather station, generate the sample weather data for given coordinates, join with the IATA/IACO codes of Airport stations and write it to sample output file.\n",
    "\n",
    "\n",
    "## Files - \n",
    "Inbound Files -  \n",
    "    Input File - \n",
    "        1. List of coordinates(latitudes and longitudes) of 400+ Australian towns. - \n",
    "        path_to_au_towns_file=\"Data/input/au-towns-sample.csv\"\n",
    "    Reference Files - \n",
    "        2. Weather Data from Weather Monitoring Stations - \n",
    "        path_to_weather_data_file= \"Data/input/weatherAUS.csv\"\n",
    "        3. IATA/IACO Codes for Airport near to Weather Stations - \n",
    "        path_to_iata_codes_file= \"Data/input/iatacodes.csv\"\n",
    "        4. Weather Station with their Latitudes and Longitudes - \n",
    "        path_to_weather_station_lat_lang_file= \"Data/input/station_lat_lang.csv\"\n",
    "        \n",
    "Outbound Files -\n",
    "        1. File in required format having sample weather data \n",
    "        output_path_to_generated_weather_data_file = \"Data/output/outputSampleWeather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boiler plate code for spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ashutosh/spark-2.3.0-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "from pyspark.sql.functions import rand,when\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime,timedelta,date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherDataGenerator\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working on local mode , so there is no sense in having shuffling to default of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and Reference files -\n",
    "path_to_au_towns_file=\"Data/input/au-towns-sample.csv\"\n",
    "path_to_weather_data_file= \"Data/input/weatherAUS.csv\"\n",
    "path_to_iata_codes_file=\"Data/input/iatacodes.csv\"\n",
    "path_to_weather_station_lat_lang_file=\"Data/input/station_lat_lang.csv\"\n",
    "## Output File -\n",
    "output_path_to_generated_weather_data_file = \"Data/output/outputSampleWeather\"\n",
    "#output_path_to_generated_weather_data_file = \"/home/ashutosh/outputSampleWeather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a dataframe of Weather Data from 49 Weather Stations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : Intentionally reading data using inferSchema = true as the data contains data as string in many rows\n",
    "### Also just to demonstrate that in most of the cases, data munging requires on the fly data casting demonstrated in the last steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: timestamp, Location: string, MinTemp: string, MaxTemp: string, Rainfall: string, Evaporation: string, Sunshine: string, WindGustDir: string, WindGustSpeed: string, WindDir9am: string, WindDir3pm: string, WindSpeed9am: string, WindSpeed3pm: string, Humidity9am: string, Humidity3pm: string, Pressure9am: string, Pressure3pm: string, Cloud9am: string, Cloud3pm: string, Temp9am: string, Temp3pm: string, RainToday: string, RISK_MM: string, RainTomorrow: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_WSD=spark.read.load(path_to_weather_data_file,\n",
    "                    format=\"csv\", \n",
    "                    sep=\",\", \n",
    "                    inferSchema=\"true\", \n",
    "                    header=\"true\")\n",
    "df_WSD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe for Weather Station with their Latitudes and Longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[City: string, latitude: double, longitude: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_WS_lat_lang=spark.read.load(path_to_weather_station_lat_lang_file,\n",
    "                     format=\"csv\", \n",
    "                    sep=\",\", \n",
    "                    inferSchema=\"true\", \n",
    "                    header=\"true\")\n",
    "df_WS_lat_lang.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create coordinate in memory table pointing to df_WS_lat_lang dataframe to run sql against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WS_lat_lang.createOrReplaceTempView(\"coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of IATA/IACO codes corresponding to all 49 weather stations , will use it while writing output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Location_Name: string, IATA_ICAO_CODE: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station_names_iata=spark.read.load(path_to_iata_codes_file,\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"True\")\n",
    "df_station_names_iata.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of 450+ Australian towns with Latitudes and Longitudes for Generating sample file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, state_code: string, latitude: double, longitude: double, elevation: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_au_towns=spark.read.load(path_to_au_towns_file,\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "df_au_towns.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numberOfSampleCities=int(input(\"Enter for many cities do you want to create sample data:  \"))\n",
    "numberOfSampleCities=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select given number of sample Cities from the list of 450+ Australian towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfCities=df_au_towns.rdd.map(lambda x:(x[0]+x[1],x[2],x[3]))\\\n",
    "                .takeSample(False, numberOfSampleCities, seed=random.randrange(1,100,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create Dataframe of randomly selected  sample list of cities/towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_towns=spark.createDataFrame(listOfCities,[\"city\",\"latitude\",\"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_towns.createOrReplaceTempView(\"targetstations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the distance of each randomly chosen town from each weather station by using harvsine formula.\n",
    "## The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.\n",
    "## Example SQL can be found on stackexchange - \n",
    "## https://stackoverflow.com/questions/11112926/how-to-find-nearest-location-using-latitude-and-longitude-from-sql-database\n",
    "## Tested the distance return by query for couple of coordinates with Google :), got close figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[city: string, latitude: double, longitude: double, target_city: string, target_latitude: double, target_longitude: double, distance: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distance=spark.sql(\"\"\"SELECT coordinates.city,\n",
    "                        coordinates.latitude,\n",
    "                        coordinates.longitude,\n",
    "                        targetstations.city as target_city,\n",
    "                        targetstations.latitude as target_latitude,\n",
    "                        targetstations.longitude as target_longitude,\n",
    "                        int(( 3959 * acos( cos( radians(targetstations.latitude) ) * \n",
    "                            cos( radians( coordinates.latitude ) ) * \n",
    "                            cos( radians( coordinates.longitude) - radians(targetstations.longitude) ) \n",
    "                            + sin( radians(targetstations.latitude) ) *  \n",
    "                            sin( radians( coordinates.latitude ) ) ) )) AS distance \n",
    "                FROM coordinates cross join  targetstations \"\"\")\n",
    "df_distance.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distance.createOrReplaceTempView(\"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark the row number based on distance of each town from all weather station.\n",
    "## Then pick the one with the least distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_closest_station_to_town = spark.sql(\"\"\"select city,\n",
    "                                    latitude,\n",
    "                                    longitude,\n",
    "                                    target_city,\n",
    "                                    round(target_latitude,2) as target_latitude,\n",
    "                                    round(target_longitude,2) as target_longitude,\n",
    "                                    distance, \n",
    "                                    row_number() over (partition by target_city order by distance)\n",
    "                                                    as least_distance\n",
    "                                    from Output having least_distance = 1\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Define Function to generate random date of 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_date=udf(lambda s: date(2016, 1, 1) + timedelta(days=int((s*1000)%365)), DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to the dataframe(closest station to town) with Random number and seed that random number to generate random date of 2016 using UDF defined at above step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cities=df_closest_station_to_town.withColumn('Random', rand())\\\n",
    "                                        .withColumn(\"Random_Date\",random_date('Random'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[city: string, latitude: double, longitude: double, target_city: string, target_latitude: double, target_longitude: double, distance: int, least_distance: int, Random: double, Random_Date: date]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_cities.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the dataframes - list of cities with weather dataframe to get the weather information for random  selected Australian towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined=df_WSD.join(list_of_cities,(df_WSD.Location==list_of_cities.city) & \n",
    "                     (df_WSD.Date.cast(\"Date\") == list_of_cities.Random_Date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on weather data fetched from weather station for that location and that date, generate the data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: timestamp, Location: string, MinTemp: string, MaxTemp: string, Rainfall: string, Evaporation: string, Sunshine: string, WindGustDir: string, WindGustSpeed: string, WindDir9am: string, WindDir3pm: string, WindSpeed9am: string, WindSpeed3pm: string, Humidity9am: string, Humidity3pm: string, Pressure9am: string, Pressure3pm: string, Cloud9am: string, Cloud3pm: string, Temp9am: string, Temp3pm: string, RainToday: string, RISK_MM: string, RainTomorrow: string, city: string, latitude: double, longitude: double, target_city: string, target_latitude: double, target_longitude: double, distance: int, least_distance: int, Random: double, Random_Date: date, Condition: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputFrame=df_joined.select(*df_joined.columns,\n",
    "        # Could have cleaned data but to show that usual data comes with garbage/blank/na/null data\n",
    "        # So handling data with NA on the fly\n",
    "                \n",
    "        # Generate the Condition column data for rainy situtations \n",
    "         when(df_joined.RainToday == 'Yes', \"Rainy\")\n",
    "        # Handling not avaialbe data in Weather Data from station\n",
    "        .when((df_joined.Rainfall!='NA')&(df_joined.Rainfall.cast(\"Float\")>20), \"Rainy\")\n",
    "        # Generate the Condition column data for Windy situtations \n",
    "        .when((df_joined.WindGustSpeed != 'NA')&(df_joined.WindGustSpeed.cast(\"Integer\") > 50), \"Windy\")\n",
    "        .when((df_joined.Pressure9am != 'NA')&(df_joined.Pressure9am.cast(\"Float\") < 100 ),\"Windy\")\n",
    "        # Handling Humid Weather      \n",
    "        .when((df_joined.Humidity9am != 'NA')&(df_joined.Humidity9am.cast(\"Float\") > 85), \"Humid\")\n",
    "        # Handling weather with Sunshine intensity\n",
    "        .when(((df_joined.Sunshine!='NA')&(df_joined.Sunshine.cast(\"Float\") <5)),\"Cloudy\")\n",
    "        .when(((df_joined.Sunshine!='NA')&(df_joined.Sunshine.cast(\"Float\") > 5)&\n",
    "               (df_joined.Sunshine.cast(\"Float\") <10)),\"Partial Cloudy\")\n",
    "        .when(((df_joined.Sunshine!='NA')&(df_joined.Sunshine.cast(\"Float\") > 10)),\"Sunny\")\n",
    "        # Defaulting col with NA\n",
    "        .otherwise(lit(\"NA\"))\\\n",
    "        .alias(\"Condition\"))\n",
    "outputFrame.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fixing for the rows having no data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFrame_filled_NA = outputFrame\\\n",
    "                            .select(*outputFrame.columns,\n",
    "                                      when((outputFrame.MinTemp == 'NA')|(outputFrame.MaxTemp == 'NA'),\"20\")\n",
    "            .otherwise(round((outputFrame.MinTemp.cast(\"Float\") + outputFrame.MaxTemp.cast(\"Float\"))/2,1) )\n",
    "                                .alias(\"AvgTemp\"), \n",
    "                                   when(outputFrame.Condition == \"NA\",\"Clear Sky\")\n",
    "                                    .otherwise(outputFrame.Condition)\n",
    "                                    .alias(\"RefinedCond\"),\n",
    "                                when(outputFrame.Pressure9am == 'NA',\"1001\")\n",
    "                                    .otherwise(outputFrame.Pressure9am)\n",
    "                                    .alias(\"Pressure\"),\n",
    "                                when(outputFrame.Humidity9am == 'NA', \"55\")\n",
    "                                    .otherwise(outputFrame.Humidity9am)\n",
    "                                    .alias(\"Humidity\")\n",
    "                                  \n",
    "                                   )\n",
    "                                                                                  \n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Broadcast the IATA codes file and join it with Outputframe having all the weather information. \n",
    "## Select required fields from the joined table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE - Use COALESCE(1) to get onle one output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile= outputFrame_filled_NA\\\n",
    "                        .join(broadcast(df_station_names_iata),\n",
    "                          outputFrame_filled_NA.Location == df_station_names_iata.Location_Name)\\\n",
    "                        .select(\"IATA_ICAO_CODE\",\n",
    "                            concat_ws(',',\"target_latitude\",\"target_longitude\"),\n",
    "                            \"Date\",\n",
    "                            \"RefinedCond\",\n",
    "                            \"AvgTemp\",\n",
    "                            \"Pressure\",\n",
    "                            \"Humidity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output file in required format\n",
    "### Field 1 - IATA/ICAO code for nearest station\n",
    "### Field 2 - Latitude,Longitude of the randomly selected town from 450 AU towns list.\n",
    "### Field 3 - Timestamp of recorded weather data, date selected randomly in year 2016\n",
    "### Field 4-  Condition like - Rainy,Humid,Cloudy,Partial Cloudy,Clear Sky, Sunny based on clues from data\n",
    "### Field 5 - Average temperature of the day \n",
    "### Field 6 - Atmospheric Pressure\n",
    "### Field 7 - Humidity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .option(\"sep\",\"|\")\\\n",
    "                .save(output_path_to_generated_weather_data_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
